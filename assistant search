#import the libiraries
from newspaper import Article
import random
import string
import nltk
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import warnings
warnings.filterwarnings('ignore')


#get the website by the keywords as inputs
import requests, sys, webbrowser, bs4


r = requests.get('https://google.com/search?q='+''.join(sys.argv[1:]))
r.raise_for_status()

soup= bs4.BeautifulSoup(r.text , "html.parser")
LinkElements = soup.select(' .r a')
LinkToOpen = min(5 , len(LinkElements))

for i in range(LinkToOpen):
    webbrowser.open('https://.google.com' + LinkElements[i].get('href'))
    
    
    #download punkt package
nltk.download ('punkt' , quiet=True)

#get the article
article = Article(r)
article.download()
article.parse()
article.nlp()
corpus = article.text
